# -*- coding: utf-8 -*-
"""Final_project(shunkah).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17-8TVc3TvbcXGP5zWDOAX_io5Jki82ne

# Final project: create an auto transcription program

Create a progam to generate transcripts from an input video. You need to submit an executable python function or class, so that in the test code, the function or class can be imported and called. Your function or class will take two inputs:
```
def auto_transcript(input_file, output_file):
    # input_file is a str contains the path of your input video file.
    # output_file is a str contains the path of the .txt file where you save your transcript.
```
- For the input video, you can record yourself speaking, or you can find a video of a person speaking online.
- The content in the output txt file should look like this:
```
[00:00:00] Bruce: Hi there, let's talk about our final project.
[00:00:05] Bruce: The final project will be an auto transcription program.
...
```

- You will need to seperate the conversation into sentences (you can accomplish it by identifying pauses in audio wave signal), and mark the time of the start of each sentence in the video.
- You will need to use face recognition to determine who is the speaker.
- You will need to perform ASR to generate transcripts. If the person speaks languages other than English, you need to detect the language and translate into English.
- You only need to consider the scenario where only 1 person is presented in the video. However, bonus points will be granted if your program can handle the case where multiple person are presented in the video and speaks in turn. You can use face landmark detection models to detect landmarks off speaker's lips, and then determine if a person's mouth is moving in the video. For example, you can use [Mediapipe](https://mediapipe.readthedocs.io/en/latest/solutions/face_mesh.html) to detect [facial landmarks](https://storage.googleapis.com/mediapipe-assets/documentation/mediapipe_face_landmark_fullsize.png). In this case you will need to output a transcript like this:
```
[00:00:00] Bruce: Hi there, let's talk about our final project.
[00:00:05] Franklin: Sure, what are we going to build for the final project?
[00:00:10] Bruce: The final project will be an auto transcription program.
```
"""

!pip install opencv-python face_recognition

!pip install pytube
import pytube
!pip install openai-whisper moviepy googletrans==4.0.0-rc1

!pip install facenet-pytorch

from facenet_pytorch import MTCNN, InceptionResnetV1
import torch
from torch.utils.data import DataLoader
from torchvision import datasets
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
from PIL import Image
import os

def download_youtube(url):
    from pytube import YouTube
    import pandas as pd
    import time

    #DOWNLOAD
    audio_folder = '/content'
    time.sleep(1)
    yt = YouTube(url)
    video_name = yt.title
    stream = yt.streams.filter(file_extension='mp4').get_highest_resolution()

    # RENAME & STORE IN LOCAL PATH
    audio_file_name = ''.join(c if c.isalnum() else '_' for c in video_name) + ".mp4"
    stream.download(output_path=audio_folder, filename=audio_file_name)

    # STORE IN DATAFRAME
    df_video = pd.DataFrame({'video_name': [audio_file_name]})

    print("Video downloaded OK")
    return audio_file_name

def collate_fn(x):
    return x[0]

def face_rec (file1):
  import cv2
  import torch
  video_capture = cv2.VideoCapture(file1)



  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
  print('Running on device: {}'.format(device))

  workers = 4 #we try to allocate 4 workers
  # Create a zip file to store faces
  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')


  skip_frames = 2
  frame_count = 0

  while video_capture.isOpened():
          ret, frame = video_capture.read()
          if not ret:
              break

            # Increment frame count
          frame_count += 1

            # Skip frames if necessary
          if frame_count % skip_frames != 0:
             continue  # Skip this frame

          # Convert frame to grayscale for face detection
          gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

          # Detect faces in the frame
          faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
          save_dir = 'saved_images'

          if len(faces) > 0:
              for i, (x, y, w, h) in enumerate(faces):
                  face_img = frame[y:y+h, x:x+w]
                  cv2.imwrite(f'face_{frame_count}.jpg', face_img)





  video_capture.release()
  cv2.destroyAllWindows()

  from facenet_pytorch import MTCNN, InceptionResnetV1

  mtcnn = MTCNN(
      image_size=160, margin=0, min_face_size=20,
      thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,
      device=device
  )

  resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)

  !unzip -q testimagesforproject.zip



  dataset = datasets.ImageFolder('testimages')

  dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}
  loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers)

  aligned = []
  names = []
  for x, y in loader:
      x_aligned, prob = mtcnn(x, return_prob=True)
      if x_aligned is not None:
          print('Face detected with probability: {:8f}'.format(prob))
          aligned.append(x_aligned)
          names.append(dataset.idx_to_class[y])


  aligned = torch.stack(aligned).to(device)
  embeddings = resnet(aligned).detach().cpu()

  print(embeddings.shape)


  dists = [[(e1 - e2).norm().item() for e2 in embeddings] for e1 in embeddings]

  query1_path = 'face_1000.jpg'
  query2_path = 'face_1002.jpg'

  # Load images using OpenCV
  query1 = cv2.imread(query1_path)
  query2 = cv2.imread(query2_path)


  query1 = Image.fromarray(cv2.cvtColor(query1, cv2.COLOR_BGR2RGB))
  query2 = Image.fromarray(cv2.cvtColor(query2, cv2.COLOR_BGR2RGB))

  face_aligned1, prob1 = mtcnn(query1, return_prob=True)
  face_aligned2, prob2 = mtcnn(query2, return_prob=True)

  aligned = torch.stack([face_aligned1, face_aligned2]).to(device)
  query_embeddings = resnet(aligned).detach().cpu()

  # compute distances of query image and images in database
  dists1 = [(e - query_embeddings[0]).norm().item() for e in embeddings]
  # find the closest one
  name = names[np.argmin(dists1)]

  return name

def extract_audio(video_path, audio_path):
    from moviepy.editor import VideoFileClip
    video = VideoFileClip(video_path)
    video.audio.write_audiofile(audio_path)

def format_time(seconds):
  import datetime
  return str(datetime.timedelta(seconds=int(seconds))).zfill(8)

def save_transcription_to_file(name, sentences_with_timestamps, output_file):
  with open(output_file, 'w', encoding='utf-8') as f:
    for sentence_info in sentences_with_timestamps:
      start_time = format_time(sentence_info['start'])
      text = sentence_info['text']
      #name = face_rec(input_file)
      f.write(f"[{start_time}] {name}: {text}\n")

def split_into_sentences_with_timestamps(result):
    segments = result['segments']
    sentences = []
    current_sentence = []
    current_start_time = None

    for segment in segments:
        words = segment['words']
        for word_info in words:
            word = word_info['word']
            start_time = word_info['start']
            end_time = word_info['end']

            if current_start_time is None:
                current_start_time = start_time

            current_sentence.append(word)

            # Check if the word ends with a sentence-ending punctuation
            if word.endswith(('.', '!', '?')):
                sentence = ' '.join(current_sentence).strip()
                sentences.append({
                    'text': sentence,
                    'start': current_start_time,
                    'end': end_time
                })
                current_sentence = []
                current_start_time = None

    # Add any remaining sentence
    if current_sentence:
        sentence = ' '.join(current_sentence).strip()
        sentences.append({
            'text': sentence,
            'start': current_start_time,
            'end': end_time
        })

    return sentences

def auto_transcript(input_file, output_file,name):
    # input_file is a str contains the path of your input video file.
    # output_file is a str contains the path of the .txt file where you save your transcript.
    video_path = input_file
    audio_path = "extracted_audio.wav"
    extract_audio(video_path, audio_path)

    import whisper

    model = whisper.load_model("base")
    result = model.transcribe(audio_path, word_timestamps= True)
    transcription = result["text"]
    print("Transcription:", transcription)

    task = 'translate'
    translated = model.transcribe(audio_path, task= task, word_timestamps= True)
    translated_text_eng = translated['text']
    print(translated_text_eng)

    from googletrans import Translator

    translator = Translator()
    translated_text = translator.translate(translated_text_eng, src='en', dest='zh-tw').text
    print("Translated Text:", translated_text)

    import nltk
    nltk.download('punkt')

    sentences_with_timestamps = split_into_sentences_with_timestamps(translated)

    # Print sentences with their timestamps
    for i, sentence_info in enumerate(sentences_with_timestamps, 1):
        print(f"Sentence {i}: {sentence_info['text']}")
        print(f"Start Time: {sentence_info['start']}")
        print(f"End Time: {sentence_info['end']}")

    output_file = "atranscription_with_timestamps.txt"
    save_transcription_to_file(name, sentences_with_timestamps, output_file)

    print(f"Transcription with timestamps saved to {output_file}")

def ifurl( x):
    if (x.startswith("https:") or x.startswith("http:") or x.startswith('www.')):
      input_file = download_youtube(x)
    else:
      input_file = x
    return input_file

def use_this():# Open the file outside the class and pass the file name to the method
  output_file_name = "output.txt"
  print("Insert URL/file of video that you would like to translate or transcibe: ")
  x = input()
  input_file = ifurl(x)
  name = face_rec(input_file)
  with open(output_file_name, "w") as f:
      auto_transcript(input_file, output_file_name,name)

use_this()